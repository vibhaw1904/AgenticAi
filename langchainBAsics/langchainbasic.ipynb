{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16119e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I should clarify that I'm not Vibhaw - I'm Claude, an AI assistant created by Anthropic. I aim to be direct and honest about who and what I am. How can I help you today?\", additional_kwargs={}, response_metadata={'id': 'msg_019Qdo7gWAjuRUXfpHkfJPpA', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 13, 'output_tokens': 50, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20241022'}, id='run--00c386b7-fb79-4edb-a16e-63c6d31163b5-0', usage_metadata={'input_tokens': 13, 'output_tokens': 50, 'total_tokens': 63, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "      os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter API key for Anthropic: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")\n",
    "\n",
    "\n",
    "model.invoke(\"Hello, vibhaw!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2fb6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To integrate a custom LLM model instead of the listed LLM models, you have several options. Here\\'s a guide on how to do it:\\n\\n1. Using Custom LLM Class:\\n```python\\nfrom langchain.llms.base import LLM\\nfrom typing import Any, List, Optional\\n\\nclass CustomLLM(LLM):\\n    def __init__(self, model_path: str):\\n        super().__init__()\\n        self.model_path = model_path\\n        # Initialize your model here\\n        self.model = load_your_model(model_path)\\n\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        # Implement your model inference logic here\\n        response = self.model.generate(prompt)\\n        return response\\n\\n    @property\\n    def _identifying_params(self) -> dict[str, Any]:\\n        return {\"model_path\": self.model_path}\\n\\n    @property\\n    def _llm_type(self) -> str:\\n        return \"custom_llm\"\\n\\n# Usage\\ncustom_llm = CustomLLM(model_path=\"path/to/your/model\")\\n```\\n\\n2. Using Local Models with LlamaCpp:\\n```python\\nfrom langchain.llms import LlamaCpp\\n\\n# Initialize local Llama model\\nllm = LlamaCpp(\\n    model_path=\"path/to/your/model.bin\",\\n    temperature=0.75,\\n    max_tokens=2000,\\n    n_ctx=2048,\\n    top_p=1\\n)\\n```\\n\\n3. Using HuggingFace Models:\\n```python\\nfrom langchain.llms import HuggingFacePipeline\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\\n\\n# Load model and tokenizer\\nmodel_id = \"path/to/your/model\"\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\\n\\n# Create pipeline\\npipe = pipeline(\\n    \"text-generation\",\\n    model=model,\\n    tokenizer=tokenizer,\\n    max_length=100\\n)\\n\\n# Create LangChain LLM\\nllm = HuggingFacePipeline(pipeline=pipe)\\n```\\n\\n4. Using Local Models with GPT4All:\\n```python\\nfrom langchain.llms import GPT4All\\n\\n# Initialize local GPT4All model\\nllm = GPT4All(model=\"path/to/your/model.bin\", n_ctx=512, n_threads=8)\\n```\\n\\n5. Integration with Chain:\\n```python\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\n\\n# Create prompt template\\nprompt = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"Question: {question}\\\\nAnswer:\"\\n)\\n\\n# Create chain with custom LLM\\nchain = LLMChain(llm=custom_llm, prompt=prompt)\\n\\n# Use the chain\\nresponse = chain.run(\"Your question here\")\\n```\\n\\n6. Using Custom Embedding Models:\\n```python\\nfrom langchain.embeddings.base import Embeddings\\n\\nclass CustomEmbeddings(Embeddings):\\n    def __init__(self, model_path: str):\\n        self.model_path = model_path\\n        self.model = load_your_embedding_model(model_path)\\n\\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        embeddings = []\\n        for text in texts:\\n            embedding = self.model.encode(text)\\n            embeddings.append(embedding)\\n        return embeddings\\n\\n    def embed_query(self, text: str) -> List[float]:\\n        return self.model.encode(text)\\n\\n# Usage\\ncustom_embeddings = CustomEmbeddings(model_path=\"path/to/your/embedding/model\")\\n```\\n\\n7. Error Handling:\\n```python\\nclass CustomLLM(LLM):\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        try:\\n            response = self.model.generate(prompt)\\n            return response\\n        except Exception as e:\\n            print(f\"Error', additional_kwargs={}, response_metadata={'id': 'msg_01VM5tcVUMmamtrxn88yN9LW', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 24, 'output_tokens': 1024, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20241022'}, id='run--4a75a68d-cf31-44f8-869a-104e8a463e43-0', usage_metadata={'input_tokens': 24, 'output_tokens': 1024, 'total_tokens': 1048, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"how can integrate custom LLm model instaed of listed llm models?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
